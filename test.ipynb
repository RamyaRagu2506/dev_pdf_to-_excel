{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import azure.functions as func\n",
    "import logging\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import os\n",
    "import pyodbc\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_input_pdf_nbd = \"pdf/processed/Account_Details_03Jun2023_133827_Emirates-NBD-Classic-Luxury-Main_2023-06-14T17:05:30.3232782Z.pdf\"\n",
    "incoming_input_pdf_cbd = \"pdf/processed/Account_Details_03Jun2023_133827_CBD_2023-06-14T17:05:30.3232782Z.pdf\"\n",
    "incoming_input_pdf_cltrak = \"pdf/processed/Account_Details_03Jun2023_133827_CLTRAK_2023-06-14T17:05:30.3232782Z.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akcserver.database.windows.net dbarunsql Arun Asds@2022 TransactionDetails\n",
      "starting point\n",
      "36     16-06-2023\n",
      "37     16-06-2023\n",
      "38     16-06-2023\n",
      "39     16-06-2023\n",
      "40     16-06-2023\n",
      "          ...    \n",
      "238    16-06-2023\n",
      "239    16-06-2023\n",
      "242    16-06-2023\n",
      "243    16-06-2023\n",
      "244    16-06-2023\n",
      "Name: TransactionDate, Length: 201, dtype: object\n",
      "report template\n",
      "Predictiing using Model path\n",
      "130499.83\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import azure.functions as func\n",
    "import logging\n",
    "from azure.storage.blob import BlobServiceClient, ContentSettings\n",
    "import os\n",
    "import pyodbc\n",
    "import re\n",
    "from datetime import datetime\n",
    "from joblib import dump, load\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "def read_user_input_data(input_file, df_input):\n",
    "    print('starting point')\n",
    "    current_datetime = datetime.now()\n",
    "    Current_Date= current_datetime.strftime(\"%Y-%m-%d\")\n",
    "    filter_date = pd.to_datetime(\"2023-6-17\").date()\n",
    "    \n",
    "    logging.info(filter_date)\n",
    "    \n",
    "    df_input['ModelDate'] = pd.to_datetime(df_input['ModelCopyDateTime'])\n",
    "    df_input['ModelDateExtracted']=df_input['ModelDate'].dt.date\n",
    "    df_input =  df_input[df_input['ModelDateExtracted']==filter_date]\n",
    "    \n",
    "    if 'Emirates-NBD-Classic-Luxury-Main' in input_file:\n",
    "        nbd_df = df_input[df_input['DomainName']=='Emirates-NBD-Classic-Luxury-Main']  \n",
    "        return nbd_df\n",
    "    \n",
    "    elif 'Rak-Bank-Classic-Luxury' in input_file:\n",
    "        clt_rak_df = df_input[df_input['DomainName']=='Rak Bank-Classic Luxury']\n",
    "        return clt_rak_df\n",
    "    \n",
    "    elif 'CLT-ADCB' in input_file:\n",
    "        clt_adcb_df = df_input[df_input['DomainName']=='CLT - ADCB']\n",
    "        return clt_adcb_df\n",
    "    \n",
    "    elif 'CBD-Bank' in input_file:\n",
    "        cbd_df = df_input[df_input['DomainName']=='CBD-Bank']\n",
    "        return cbd_df\n",
    "    \n",
    "    elif 'EIB-Loan-account' in input_file:\n",
    "        cbd_df = df_input[df_input['DomainName']=='EIB - Loan account']\n",
    "        return cbd_df\n",
    "    \n",
    "    elif 'OLT-Emirates-Islamic-Bank' in input_file:\n",
    "        cbd_df = df_input[df_input['DomainName']=='OLT - Emirates Islamic Bank']\n",
    "        return cbd_df\n",
    "\n",
    "    \n",
    "    elif 'Emirates-NBD-Classic-Passenger' in input_file:\n",
    "        emirates_nbd_classic_passenger_df= df_input[df_input['DomainName']=='Emirates NBD-Classic Passenger']\n",
    "        return emirates_nbd_classic_passenger_df\n",
    "    \n",
    "    elif 'ENBD-Classic-Riders' in input_file:\n",
    "        enbd_classic_riders_df = df_input[df_input['DomainName']=='ENBD - Classic Riders']\n",
    "        return enbd_classic_riders_df\n",
    "    \n",
    "    else: \n",
    "        print(f'{input_file} Does not exist.')\n",
    "        \n",
    "def general_preprocess(input_df):\n",
    "    if input_df['TransactionDate'].isnull().sum() > 0:\n",
    "        dropped_df = input_df['TransactionDate'].dropna()\n",
    "        print(dropped_df)\n",
    "        return input_df\n",
    "    return input_df\n",
    "\n",
    "def preprocess_template_data(input_dataframe):\n",
    "    lowered_description = input_dataframe['Description'].apply(str.lower)\n",
    "    input_dataframe['description_lowered'] = lowered_description\n",
    "    return input_dataframe\n",
    "\n",
    "def preprocess_text_data(input_df):\n",
    "    lowered_narration_data_series = input_df['Narration'].apply(lambda x: str(x).lower() if x is not None else None)\n",
    "    input_df['lowered_narration'] = lowered_narration_data_series\n",
    "    return input_df\n",
    "\n",
    "def predict_transactions(new_transactions, model_weights, vectorizer_weights):\n",
    "    # Load the saved model\n",
    "\n",
    "    # Vectorize the new data\n",
    "    new_transactions_vectorized = vectorizer_weights.transform(new_transactions)\n",
    "\n",
    "    # Predict using the loaded model\n",
    "    predictions = model_weights.predict(new_transactions_vectorized)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "def fetch_data_from_sql(server, database, username, password, table_name):\n",
    "    # Establish the database connection\n",
    "    print(server, database, username, password, table_name)\n",
    "    conn_str = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "    \n",
    "    conn = pyodbc.connect(conn_str)\n",
    "\n",
    "    # Read rows from the SQL table\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Convert the result to a DataFrame\n",
    "    df =pd.DataFrame([tuple(row) for row in rows], columns=[column[0] for column in cursor.description])\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def populate_final_report(report_template, nlp_classified_df, input_file_path):\n",
    "    server = \"akcserver.database.windows.net\"\n",
    "    database = \"dbarunsql\"\n",
    "    username = \"Arun\" \n",
    "    password = \"Asds@2022\"\n",
    "    driver = '{ODBC Driver 17 for SQL Server}'  # Update the driver if needed\n",
    "    \n",
    "    conn_str = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    closingbalanceTableName = \"DailyTransactionBalance\"\n",
    "    openingBalance = list(pdf_based_file_preprocessed_data['RunningBalance'])[0]\n",
    "    \n",
    "    if 'Emirates-NBD-Classic-Luxury-Main' in input_file_path:\n",
    "        \n",
    "        for description in report_template['Description']:\n",
    "            filtered_df = nlp_classified_df[nlp_classified_df['Prediction'] == description]\n",
    "            debit_sum = filtered_df['Debit'].sum()\n",
    "            credit_sum = filtered_df['Credit'].sum()\n",
    "            total_sum = credit_sum + (-debit_sum)\n",
    "            report_template.loc[report_template['Description'] == description, 'Emirates NBD-Classic Luxury-Main'] = total_sum\n",
    "        closingBalance = report_template['Emirates NBD-Classic Luxury-Main'][1:12].sum()\n",
    "        report_template.loc[report_template['Description'] == 'Closing Balance at the day end', 'Emirates NBD-Classic Luxury-Main'] = report_template['Emirates NBD-Classic Luxury-Main'][1:12].sum()\n",
    "        report_template.loc[report_template['Description'] == 'Opening Balance', 'Emirates NBD-Classic Luxury-Main'] = openingBalance\n",
    "        cursor = conn.cursor()\n",
    "        sql = f\"INSERT INTO {closingbalanceTableName} (ClosingBalanceDomainCompany, CreatedDate, ModifiedDate, ClosingBalance) \" \\\n",
    "              f\"VALUES (?, ?, ?, ?)\"\n",
    "\n",
    "# Prepare the values for the parameters\n",
    "        params = ('Emirates-NBD-Classic-Luxury-Main', datetime.now(), datetime.now(), closingBalance)\n",
    "\n",
    "# Execute the SQL statement with the parameters\n",
    "        cursor.execute(sql, params)\n",
    "        conn.commit()\n",
    "        conn.close\n",
    "        \n",
    "        return report_template\n",
    "    elif 'CBD-Bank' in input_file_path:\n",
    "        \n",
    "        for description in report_template['Description']:\n",
    "            filtered_df = nlp_classified_df[nlp_classified_df['Prediction'] == description]\n",
    "            debit_sum = filtered_df['Debit'].sum()\n",
    "            credit_sum = filtered_df['Credit'].sum()\n",
    "            total_sum = credit_sum + (-debit_sum)\n",
    "            report_template.loc[report_template['Description'] == description, 'CBD Bank'] = total_sum\n",
    "        report_template.loc[report_template['Description'] == 'Closing Balance at the day end', 'CBD Bank'] = report_template['CBD Bank'][1:12].sum()\n",
    "        return report_template\n",
    "    elif 'Rak-Bank-Classic-Luxury' in input_file_path:\n",
    "        \n",
    "        for description in report_template['Description']:\n",
    "            filtered_df = nlp_classified_df[nlp_classified_df['Prediction'] == description]\n",
    "            debit_sum = filtered_df['Debit'].sum()\n",
    "            credit_sum = filtered_df['Credit'].sum()\n",
    "            total_sum = credit_sum + (-debit_sum)\n",
    "            report_template.loc[report_template['Description'] == description, 'Rak Bank-Classic Luxury'] = total_sum\n",
    "        report_template.loc[report_template['Description'] == 'Closing Balance at the day end', 'Rak Bank-Classic Luxury'] = report_template['Rak Bank-Classic Luxury'][1:12].sum()\n",
    "        return report_template\n",
    "    elif 'CLT-ADCB' in input_file_path:\n",
    "        \n",
    "        for description in report_template['Description']:\n",
    "            filtered_df = nlp_classified_df[nlp_classified_df['Prediction'] == description]\n",
    "            debit_sum = filtered_df['Debit'].sum()\n",
    "            credit_sum = filtered_df['Credit'].sum()\n",
    "            total_sum = credit_sum + (-debit_sum)\n",
    "            report_template.loc[report_template['Description'] == description, 'CLT-ADCB'] = total_sum\n",
    "        report_template.loc[report_template['Description'] == 'Closing Balance at the day end', 'CLT-ADCB'] = report_template['CLT-ADCB'][1:12].sum()\n",
    "        return report_template\n",
    "    elif 'EIB-Loan-account' in input_file_path:\n",
    "        \n",
    "        for description in report_template['Description']:\n",
    "            filtered_df = nlp_classified_df[nlp_classified_df['Prediction'] == description]\n",
    "            debit_sum = filtered_df['Debit'].sum()\n",
    "            credit_sum = filtered_df['Credit'].sum()\n",
    "            total_sum = credit_sum + (-debit_sum)\n",
    "            report_template.loc[report_template['Description'] == description, 'EIB-Loan account'] = total_sum\n",
    "        report_template.loc[report_template['Description'] == 'Closing Balance at the day end', 'EIB-Loan account'] = report_template['EIB-Loan account'][1:12].sum()\n",
    "        return report_template\n",
    "    elif 'OLT-Emirates-Islamic-Bank' in input_file_path:\n",
    "        \n",
    "        for description in report_template['Description']:\n",
    "            filtered_df = nlp_classified_df[nlp_classified_df['Prediction'] == description]\n",
    "            debit_sum = filtered_df['Debit'].sum()\n",
    "            credit_sum = filtered_df['Credit'].sum()\n",
    "            total_sum = credit_sum + (-debit_sum)\n",
    "            report_template.loc[report_template['Description'] == description, 'OLT - Emirates Islamic Bank'] = total_sum\n",
    "        report_template.loc[report_template['Description'] == 'Closing Balance at the day end', 'OLT - Emirates Islamic Bank'] = report_template['OLT - Emirates Islamic Bank'][1:12].sum()\n",
    "        return report_template\n",
    "    elif 'Emirates-NBD-Classic-Passenger' in input_file_path:\n",
    "        for description in report_template['Description']:\n",
    "            filtered_df = nlp_classified_df[nlp_classified_df['Prediction'] == description]\n",
    "            debit_sum = filtered_df['Debit'].sum()\n",
    "            credit_sum = filtered_df['Credit'].sum()\n",
    "            total_sum = credit_sum + (-debit_sum)\n",
    "            report_template.loc[report_template['Description'] == description, 'Emirates-NBD-Classic-Passenger'] = total_sum\n",
    "        report_template.loc[report_template['Description'] == 'Closing Balance at the day end', 'Emirates-NBD-Classic-Passenger'] = report_template['Emirates-NBD-Classic-Passenger'][1:12].sum()\n",
    "        return report_template\n",
    "    elif 'ENBD-Classic-Riders' in input_file:\n",
    "        for description in report_template['Description']:\n",
    "            filtered_df = nlp_classified_df[nlp_classified_df['Prediction'] == description]\n",
    "            debit_sum = filtered_df['Debit'].sum()\n",
    "            credit_sum = filtered_df['Credit'].sum()\n",
    "            total_sum = credit_sum + (-debit_sum)\n",
    "            report_template.loc[report_template['Description'] == description, 'OENBD - Classic Riders'] = total_sum\n",
    "        closingBalance = report_template['ENBD - Classic Riders'][1:12].sum()\n",
    "        report_template.loc[report_template['Description'] == 'Closing Balance at the day end', 'ENBD - Classic Riders'] = report_template['ENBD - Classic Riders'][1:12].sum() \n",
    "        \n",
    "        return report_template\n",
    "\n",
    "def update_sql_table_for_classified(pdf_based_file_preprocessed_data, server, database, username, password, table_name):\n",
    "    # Load the dataframe from Azure SQL or any other data source\n",
    "    # For the purpose of this example, let's assume the dataframe is already loaded\n",
    "    # into a variable named \"pdf_based_file_preprocessed_data\"\n",
    "\n",
    "    # Check if the 'Classified' column is 'No'\n",
    "    condition = (pdf_based_file_preprocessed_data['Classified'] == 'No')\n",
    "\n",
    "    # Filter the dataframe to get only the rows where the condition is True\n",
    "    filtered_df = pdf_based_file_preprocessed_data.loc[condition]\n",
    "\n",
    "    # Get the IDs of the rows where the condition is True\n",
    "    ids_to_update = filtered_df['TransactionId'].tolist()\n",
    "\n",
    "    # Create the connection string\n",
    "    conn_str = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "\n",
    "    # Connect to Azure SQL\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Update the SQL table for each ID\n",
    "    for id_to_update in ids_to_update:\n",
    "        update_query = f\"UPDATE {table_name} SET Classified = 'Yes' WHERE TransactionId = {id_to_update}\"\n",
    "        cursor.execute(update_query)\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "    # Return the list of IDs that were updated\n",
    "    return ids_to_update\n",
    "\n",
    "def save_dataframe_to_blob(dataframe, connection_string, container_name, excel_file_name):\n",
    "    # Save DataFrame to Excel file\n",
    "    report_file = BytesIO()\n",
    "    dataframe.to_excel(report_file, index=False)\n",
    "    report_file.seek(0)\n",
    "    \n",
    "    \n",
    "    # Upload Excel file to Blob storage\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_client = container_client.get_blob_client(excel_file_name)\n",
    "\n",
    "    \n",
    "    blob_client.upload_blob(report_file, overwrite=True, content_settings=ContentSettings(content_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'))\n",
    "\n",
    "def load_nlp_models(connection_string, data_frame):\n",
    "    # Remove rows with null values in the 'Narration' column\n",
    "    nlp_classified = data_frame.dropna(subset=['Narration'])\n",
    "    nlp_classified_data_without_nulls = nlp_classified.dropna(subset=['Narration'])\n",
    "    nlp_bank_transactions = nlp_classified_data_without_nulls['Narration']\n",
    "\n",
    "    # Load the NLP models from Azure Storage\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    container_client = blob_service_client.get_container_client(\"akcsaiamodel\")\n",
    "\n",
    "    # Download the model blob\n",
    "    model_blob_client = container_client.get_blob_client(\"AkcsNlpCustommodel_V1.pkl\")\n",
    "    model_blob_data = model_blob_client.download_blob().readall()\n",
    "    model_weights = pickle.loads(model_blob_data)\n",
    "\n",
    "    # Download the vectorizer blob\n",
    "    vectorizer_blob_client = container_client.get_blob_client(\"Vectorizer_V1.pkl\")\n",
    "    vectorizer_blob_data = vectorizer_blob_client.download_blob().readall()\n",
    "    vectorizer_weights = pickle.loads(vectorizer_blob_data)\n",
    "\n",
    "    return nlp_bank_transactions, model_weights, vectorizer_weights\n",
    "\n",
    "def insert_data_into_training_table(server, database, username, password):\n",
    "    \n",
    "    conn_str = f\"DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "    unclassified_df = pdf_based_file_preprocessed_data[['Narration','Prediction']]\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    unclassifiedDatatable = \"TrainingDataBankStatements\"\n",
    "    cursor = conn.cursor()\n",
    "    sql = f\"INSERT INTO {unclassifiedDatatable} (Narration, Label, CreatedDate) \" \\\n",
    "              f\"VALUES (?, ?, ?)\"\n",
    "\n",
    "# Prepare the values for the parameters\n",
    "    params = []\n",
    "    for index, row in unclassified_df.iterrows():\n",
    "        narration = row['Narration']\n",
    "        prediction = row['Prediction']\n",
    "        created_date = datetime.now()\n",
    "        params.append((narration, prediction, created_date))\n",
    "\n",
    "# Execute the SQL statement with the parameters\n",
    "    cursor.executemany(sql, params)\n",
    "    conn.commit()\n",
    "    conn.close\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "input_file = incoming_input_pdf_nbd\n",
    "template_file_path = \"https://arunakcs.blob.core.windows.net/excelfiles/main_template/test_template.xlsx\"\n",
    "df_template = pd.read_excel(template_file_path)\n",
    "df_reference = pd.read_excel(template_file_path, sheet_name=\"term_references\")\n",
    "logging.info(df_template.columns)\n",
    "\n",
    "server = \"akcserver.database.windows.net\"\n",
    "database = \"dbarunsql\"\n",
    "username = \"Arun\" \n",
    "password = \"Asds@2022\"\n",
    "table_name = \"TransactionDetails\"\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=arunakcs;AccountKey=nx8T5960W1vcaeHKOD/4HtiCm0/n58VXhtsNAp7LoyDdZX6IdRPsomJsBoOgB72wPd9AHfwwcoFo+AStndZq2Q==;EndpointSuffix=core.windows.net\"    \n",
    "try:\n",
    "    df_input_bank_statement_from_sql = fetch_data_from_sql(server, database, username, password, table_name)\n",
    "\n",
    "    df_input_bank_statement = read_user_input_data(incoming_input_pdf_nbd ,df_input_bank_statement_from_sql)\n",
    "        \n",
    "    preprocessed_data = general_preprocess(df_input_bank_statement)\n",
    "        \n",
    "    pdf_based_file_preprocessed_data = preprocess_text_data(preprocessed_data)\n",
    "        \n",
    "    report_template = preprocess_template_data(df_template)\n",
    "    print(f\"report template\")\n",
    "    \n",
    "    nlp_bank_transactions, model_weights, vectorizer_weights = load_nlp_models(connection_string, pdf_based_file_preprocessed_data)\n",
    "    \n",
    "    print(\"Predictiing using Model path\")\n",
    "    predictions = predict_transactions(nlp_bank_transactions, model_weights, vectorizer_weights)\n",
    "        \n",
    "    pdf_based_file_preprocessed_data['Prediction'] = predictions\n",
    "    \n",
    "    #fetching unclassified data and saving it in DB\n",
    "    insert_data_into_training_table(server, database, username, password)\n",
    "    \n",
    "    #update sql table\n",
    "    update_sql_table_for_classified(pdf_based_file_preprocessed_data, server, database, username, password, table_name)\n",
    "\n",
    "    print(list(pdf_based_file_preprocessed_data['RunningBalance'])[0])\n",
    "    populate_report_template = populate_final_report(report_template, pdf_based_file_preprocessed_data, incoming_input_pdf_nbd)\n",
    "    #print(populate_report_template)\n",
    "    #now_date = datetime.now()\n",
    "    #container_name = \"outputreport\"\n",
    "    #file_name = f\"output_report_{now_date.date()}_{now_date.minute}_{now_date.second}.xlsx\"\n",
    "    #print(file_name)    \n",
    "    #save_dataframe_to_blob(populate_report_template,connection_string, container_name, file_name)\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(e)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = 'functionapp-pdf'\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=arunakcs;AccountKey=nx8T5960W1vcaeHKOD/4HtiCm0/n58VXhtsNAp7LoyDdZX6IdRPsomJsBoOgB72wPd9AHfwwcoFo+AStndZq2Q==;EndpointSuffix=core.windows.net\"\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "blobs = container_client.list_blobs()\n",
    "latest_blob = max(blobs, key=lambda blob: blob.last_modified)\n",
    "latest_blob_name = latest_blob.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import csv\n",
    "\n",
    "\n",
    "driver = '{ODBC Driver 17 for SQL Server}'  # Use the appropriate ODBC driver\n",
    "\n",
    "# Establish a connection\n",
    "connection_string = f\"DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    "conn = pyodbc.connect(connection_string)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Specify the CSV file path\n",
    "csv_file_path = 'labelled.csv'\n",
    "\n",
    "# Read data from the CSV file and insert into the table\n",
    "with open(csv_file_path, 'r') as file:\n",
    "    csv_data = csv.reader(file)\n",
    "    next(csv_data)  # Skip the header row if present\n",
    "\n",
    "    for row in csv_data:\n",
    "        # Assuming the CSV columns match the table columns in order\n",
    "        column1_value = row[0]\n",
    "        column2_value = row[1]\n",
    "\n",
    "        # Build and execute the INSERT query\n",
    "        insert_query = f\"INSERT INTO TrainingDataBankStatements (Narration, Label, CreatedDate) VALUES (?, ?, ?)\"\n",
    "        cursor.execute(insert_query, (column1_value, column2_value, datetime.now()))\n",
    "\n",
    "# Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
